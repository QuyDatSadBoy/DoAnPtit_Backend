#!/usr/bin/env python3
"""
Script inference cho mÃ´ hÃ¬nh X-ray2CTPA
Chuyá»ƒn Ä‘á»•i áº£nh X-ray 2D thÃ nh CTPA 3D vá»›i xá»­ lÃ½ GIá»NG Há»†T TRAINING CODE Gá»C
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
import argparse
from PIL import Image
import cv2
from typing import Optional, Union, Tuple
import warnings
warnings.filterwarnings('ignore')

# Import cÃ¡c module cáº§n thiáº¿t
from ddpm import Unet3D, GaussianDiffusion
from ddpm.unet import UNet
import SimpleITK as sitk
from diffusers import AutoencoderKL
from torchvision import transforms as T

# ThÃªm constants tá»« training code - GIá»NG Há»†T LIDC DATASET (vÃ¬ config dÃ¹ng name_dataset: LIDC)
CONTRAST_HU_MIN = -1200.0  # Tá»« preprocess/preprocess_lidc.py line 22
CONTRAST_HU_MAX = 600.0    # Tá»« preprocess/preprocess_lidc.py line 23

# Import video_tensor_to_gif function tá»« training code - GIá»NG Há»†T REPO Gá»C
def video_tensor_to_gif(tensor, path, duration=120, loop=0, optimize=True):
    """
    Convert tensor thÃ nh GIF animation GIá»NG Há»†T REPO Gá»C
    Tensor format: (channels, frames, height, width)
    Sá»­ dá»¥ng normalization GIá»NG Há»†T ddpm/diffusion.py line 1200
    """
    # Normalize GIá»NG Há»†T repo gá»‘c: tensor.min() -> tensor.max() thÃ nh 0->1
    tensor = ((tensor - tensor.min()) / (tensor.max() - tensor.min())) * 1.0
    
    # Chuyá»ƒn thÃ nh PIL images
    images = map(T.ToPILImage(), tensor.unbind(dim=1))
    first_img, *rest_imgs = images
    first_img.save(path, save_all=True, append_images=rest_imgs,
                   duration=duration, loop=loop, optimize=optimize)
    return images

def denormalize_ctpa_volume(volume, dataset_min, dataset_max):
    """
    Denormalize CTPA volume tá»« latent space vá» HU values thá»±c táº¿
    GIá»NG Há»†T LIDC training pipeline tá»« preprocess/preprocess_lidc.py
    """
    # Model output thÆ°á»ng á»Ÿ range [-1, 1] tá»« training
    # TrÆ°á»›c tiÃªn chuyá»ƒn tá»« [-1, 1] vá» [0, 1]
    volume_01 = (volume + 1.0) / 2.0
    
    # Sau Ä‘Ã³ denormalize vá» range gá»‘c cá»§a dataset
    volume_denorm = volume_01 * (dataset_max - dataset_min) + dataset_min
    
    # Cuá»‘i cÃ¹ng denormalize tá»« dataset range vá» HU range theo LIDC
    # Reverse cá»§a: img = (img - CONTRAST_HU_MIN) / (CONTRAST_HU_MAX - CONTRAST_HU_MIN)
    volume_hu = volume_denorm * (CONTRAST_HU_MAX - CONTRAST_HU_MIN) + CONTRAST_HU_MIN
    
    # Clip vá» range HU há»£p lÃ½ cho LIDC
    volume_hu = np.clip(volume_hu, CONTRAST_HU_MIN, CONTRAST_HU_MAX)
    
    return volume_hu.astype(np.float32)

def denormalize_for_display(volume):
    """
    Denormalize volume Ä‘á»ƒ hiá»ƒn thá»‹, giá»¯ nguyÃªn range Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u
    """
    # Náº¿u volume trong range [-1, 1], chuyá»ƒn vá» [0, 1]
    if volume.min() >= -1.1 and volume.max() <= 1.1:
        volume_display = (volume + 1.0) / 2.0
        volume_display = np.clip(volume_display, 0.0, 1.0)
        return volume_display
    # Náº¿u Ä‘Ã£ trong range [0, 1], giá»¯ nguyÃªn
    elif volume.min() >= -0.1 and volume.max() <= 1.1:
        return np.clip(volume, 0.0, 1.0)
    # Náº¿u range khÃ¡c, normalize vá» [0, 1]
    else:
        volume_display = (volume - volume.min()) / (volume.max() - volume.min())
        return volume_display

def apply_medical_orientation(volume, use_training_transpose=False):
    """
    Ãp dá»¥ng orientation GIá»NG FILE generated_ctpa_none (KHÃ”NG TRANSPOSE)
    
    Args:
        volume: Input volume array
        use_training_transpose: False Ä‘á»ƒ dÃ¹ng orientation giá»‘ng generated_ctpa_none
    """
    if len(volume.shape) == 3:
        d, h, w = volume.shape
        print(f"ğŸ” Input volume shape: (D={d}, H={h}, W={w})")
        
        if use_training_transpose:
            # Training code transpose (khÃ´ng dÃ¹ng ná»¯a)
            volume_oriented = volume.transpose(2, 1, 0)  # (D,H,W) -> (W,H,D)
            print(f"ğŸ“Š Training transpose: {volume.shape} â†’ {volume_oriented.shape} (training code)")
        else:
            # GIá»NG FILE generated_ctpa_none - KHÃ”NG TRANSPOSE
            volume_oriented = volume
            print(f"ğŸ“Š No transpose: keeping original shape {volume.shape} (GIá»NG generated_ctpa_none)")
            
        return volume_oriented
    return volume

def create_medical_nifti(volume, spacing=(1.0, 1.0, 1.0)):
    """
    Táº¡o NIfTI image vá»›i metadata chuáº©n medical imaging GIá»NG TRAINING CODE
    """
    # Táº¡o NIfTI image
    nifti_image = sitk.GetImageFromArray(volume)
    
    # Set spacing (voxel size) - GIá»NG training code
    nifti_image.SetSpacing(spacing)
    
    # Set origin (tá»a Ä‘á»™ gá»‘c) - GIá»NG training code
    nifti_image.SetOrigin([0.0, 0.0, 0.0])
    
    # Set direction (hÆ°á»›ng cá»§a cÃ¡c trá»¥c) - GIá»NG training code
    # Identity matrix cho standard orientation
    nifti_image.SetDirection([1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0])
    
    return nifti_image

class XrayToCTPAInference:
    """Class Ä‘á»ƒ inference tá»« X-ray sang CTPA vá»›i xá»­ lÃ½ GIá»NG Há»†T TRAINING PIPELINE"""
    
    def __init__(
        self,
        model_checkpoint: str,
        model_config: dict,
        device: str = 'cuda'
    ):
        """
        Khá»Ÿi táº¡o inference model
        
        Args:
            model_checkpoint: ÄÆ°á»ng dáº«n Ä‘áº¿n checkpoint Ä‘Ã£ train
            model_config: Cáº¥u hÃ¬nh model
            device: Device Ä‘á»ƒ cháº¡y ('cuda' hoáº·c 'cpu')
        """
        self.device = device
        self.config = model_config
        
        # Khá»Ÿi táº¡o model
        self.model = self._load_model(model_checkpoint)
        self.model.eval()
        
        print(f"âœ… ÄÃ£ load model tá»«: {model_checkpoint}")
        print(f"ğŸ”§ Device: {device}")
        print(f"ğŸ“Š Dataset range: [{model_config.get('dataset_min_value', -12.911299):.3f}, {model_config.get('dataset_max_value', 9.596558):.3f}]")
        
    def _load_model(self, checkpoint_path: str):
        """Load model tá»« checkpoint"""
        
        # Táº¡o model architecture
        if self.config['denoising_fn'] == 'Unet3D':
            unet = Unet3D(
                dim=self.config['diffusion_img_size'],
                cond_dim=self.config.get('cond_dim', 512),
                dim_mults=self.config['dim_mults'],
                channels=self.config['diffusion_num_channels'],
                resnet_groups=8,
                classifier_free_guidance=self.config.get('classifier_free_guidance', False),
                medclip=self.config.get('medclip', True)
            ).to(self.device)
        elif self.config['denoising_fn'] == 'UNet':
            unet = UNet(
                in_ch=self.config['diffusion_num_channels'],
                out_ch=self.config['diffusion_num_channels'],
                spatial_dims=3
            ).to(self.device)
        else:
            raise ValueError(f"Model {self.config['denoising_fn']} khÃ´ng Ä‘Æ°á»£c há»— trá»£")
        
        # Táº¡o diffusion model
        diffusion = GaussianDiffusion(
            unet,
            vqgan_ckpt=self.config.get('vqgan_ckpt'),
            vae_ckpt=self.config.get('vae_ckpt'),
            image_size=self.config['diffusion_img_size'],
            num_frames=self.config['diffusion_depth_size'],
            channels=self.config['diffusion_num_channels'],
            timesteps=self.config.get('timesteps', 1000),
            img_cond=True,
            loss_type=self.config.get('loss_type', 'l1'),
            l1_weight=self.config.get('l1_weight', 1.0),
            perceptual_weight=self.config.get('perceptual_weight', 0.0),
            discriminator_weight=self.config.get('discriminator_weight', 0.0),
            classification_weight=self.config.get('classification_weight', 0.0),
            classifier_free_guidance=self.config.get('classifier_free_guidance', False),
            medclip=self.config.get('medclip', True),
            name_dataset=self.config.get('name_dataset', 'CTPA'),
            dataset_min_value=self.config.get('dataset_min_value', -12.911299),
            dataset_max_value=self.config.get('dataset_max_value', 9.596558),
        ).to(self.device)
        
        # Load checkpoint
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # Thá»­ load state dict
            if 'model' in checkpoint:
                diffusion.load_state_dict(checkpoint['model'], strict=False)
                print(f"âœ… Loaded model state dict tá»« checkpoint")
            elif 'ema' in checkpoint:
                diffusion.load_state_dict(checkpoint['ema'], strict=False)
                print(f"âœ… Loaded EMA model state dict tá»« checkpoint")
            else:
                diffusion.load_state_dict(checkpoint, strict=False)
                print(f"âœ… Loaded state dict trá»±c tiáº¿p tá»« checkpoint")
                
        else:
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y checkpoint: {checkpoint_path}")
        
        return diffusion
    
    def preprocess_xray(self, xray_path: str) -> torch.Tensor:
        """
        Tiá»n xá»­ lÃ½ áº£nh X-ray theo training pipeline
        """
        if xray_path.endswith('.npy'):
            # Load tá»« file numpy
            xray = np.load(xray_path).astype(np.float32)
        else:
            # Load tá»« áº£nh thÃ´ng thÆ°á»ng
            xray = cv2.imread(xray_path, cv2.IMREAD_GRAYSCALE)
            xray = cv2.resize(xray, (224, 224))  # Resize theo kÃ­ch thÆ°á»›c training
            xray = xray.astype(np.float32) / 255.0  # Normalize
        
        # Chuyá»ƒn thÃ nh tensor
        xray_tensor = torch.from_numpy(xray).float()
        
        # ThÃªm batch dimension náº¿u cáº§n
        if len(xray_tensor.shape) == 2:
            xray_tensor = xray_tensor.unsqueeze(0).unsqueeze(0)
        elif len(xray_tensor.shape) == 3:
            xray_tensor = xray_tensor.unsqueeze(0)
            
        return xray_tensor.to(self.device)
    
    def generate_ctpa(
        self, 
        xray_tensor: torch.Tensor,
        guidance_scale: float = 1.0,
        num_inference_steps: Optional[int] = None
    ) -> torch.Tensor:
        """
        Generate CTPA tá»« X-ray theo training pipeline
        """
        print("ğŸ”„ Äang generate CTPA...")
        
        with torch.no_grad():
            # Generate CTPA theo training pipeline
            ctpa = self.model.sample(
                cond=xray_tensor,
                cond_scale=guidance_scale,
                batch_size=xray_tensor.shape[0]
            )
            
        print("âœ… ÄÃ£ generate CTPA thÃ nh cÃ´ng!")
        return ctpa
    
    def postprocess_ctpa(self, ctpa_tensor: torch.Tensor) -> np.ndarray:
        """
        Háº­u xá»­ lÃ½ CTPA tensor thÃ nh numpy array theo training pipeline
        """
        # Chuyá»ƒn vá» CPU vÃ  numpy
        ctpa_np = ctpa_tensor.cpu().numpy()
        
        print(f"ğŸ” DEBUG: Raw tensor shape: {ctpa_tensor.shape}")
        print(f"ğŸ” DEBUG: Raw numpy shape: {ctpa_np.shape}")
        
        # Loáº¡i bá» batch dimension
        if len(ctpa_np.shape) == 5:  # (B, C, D, H, W)
            print(f"ğŸ” DEBUG: 5D tensor - (B, C, D, H, W) format")
            ctpa_np = ctpa_np[0, 0]  # Láº¥y batch Ä‘áº§u tiÃªn vÃ  channel Ä‘áº§u tiÃªn
            print(f"ğŸ” DEBUG: After removing batch & channel: {ctpa_np.shape} - should be (D, H, W)")
        elif len(ctpa_np.shape) == 4:  # (B, D, H, W) hoáº·c (C, D, H, W)
            print(f"ğŸ” DEBUG: 4D tensor - (B, D, H, W) or (C, D, H, W) format")
            ctpa_np = ctpa_np[0]
            print(f"ğŸ” DEBUG: After removing first dimension: {ctpa_np.shape} - should be (D, H, W)")
        elif len(ctpa_np.shape) == 3:  # (D, H, W)
            print(f"ğŸ” DEBUG: 3D tensor - already (D, H, W) format")
        
        print(f"ğŸ“Š CTPA shape sau postprocess: {ctpa_np.shape}")
        print(f"ğŸ“Š CTPA range trÆ°á»›c xá»­ lÃ½: [{ctpa_np.min():.3f}, {ctpa_np.max():.3f}]")
        
        # ThÃªm thÃ´ng tin vá» axes Ä‘á»ƒ debug orientation
        if len(ctpa_np.shape) == 3:
            d, h, w = ctpa_np.shape
            print(f"ğŸ” DEBUG: Interpreting as (Depth={d}, Height={h}, Width={w})")
            print(f"ğŸ” DEBUG: Sáº½ Ã¡p dá»¥ng transpose GIá»NG TRAINING CODE")
            
        return ctpa_np
    
    def save_results(
        self,
        ctpa_volume: np.ndarray,
        output_dir: str,
        filename: str = "generated_ctpa",
        formats: list = ['npy', 'nii', 'png', 'gif']
    ):
        """
        LÆ°u káº¿t quáº£ CTPA vá»›i xá»­ lÃ½ GIá»NG Há»†T TRAINING CODE Gá»C
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)
        
        # Láº¥y dataset parameters tá»« config
        dataset_min = self.config.get('dataset_min_value', -12.911299)
        dataset_max = self.config.get('dataset_max_value', 9.596558)
        
        if 'npy' in formats:
            # LÆ°u raw data vá»›i orientation gá»‘c
            np.save(output_path / f"{filename}_raw.npy", ctpa_volume)
            print(f"âœ… ÄÃ£ lÆ°u {filename}_raw.npy")
        
        if 'nii' in formats:
            # ÃP Dá»¤NG ORIENTATION GIá»NG FILE generated_ctpa_none
            print("ğŸ”„ Äang xá»­ lÃ½ CTPA GIá»NG FILE generated_ctpa_none...")
            
            # 1. Ãp dá»¥ng orientation GIá»NG generated_ctpa_none (KHÃ”NG transpose)
            ctpa_oriented = apply_medical_orientation(ctpa_volume, use_training_transpose=False)
            print(f"ğŸ“Š Orientation: NO transpose - GIá»NG generated_ctpa_none")
            
            # 2. Clip vá» range HU GIá»NG training code Ä‘á»ƒ giá»¯ cháº¥t lÆ°á»£ng
            ctpa_oriented = np.clip(ctpa_oriented, -1024, 3071)  # Standard CT HU range
            ctpa_oriented = ctpa_oriented.astype(np.float32)
            print(f"ğŸ“Š HU range: [-1024, 3071] - giá»¯ cháº¥t lÆ°á»£ng nhÆ° training code")
            
            # 3. Táº¡o NIfTI vá»›i metadata chuáº©n
            nifti_image = create_medical_nifti(ctpa_oriented, spacing=(1.0, 1.0, 1.0))
            
            # 4. LÆ°u file NIfTI chÃ­nh
            nii_path = output_path / f"{filename}_none_style.nii.gz"
            sitk.WriteImage(nifti_image, str(nii_path))
            print(f"âœ… ÄÃ£ lÆ°u {filename}_none_style.nii.gz (GIá»NG generated_ctpa_none)")
            
            # 5. LÆ°u thÃªm version vá»›i denormalize Ä‘á»ƒ so sÃ¡nh náº¿u cáº§n
            if True:  # CÃ³ thá»ƒ táº¯t Ä‘á»ƒ tiáº¿t kiá»‡m dung lÆ°á»£ng
                ctpa_denorm = denormalize_ctpa_volume(ctpa_volume, dataset_min, dataset_max)
                ctpa_denorm_oriented = apply_medical_orientation(ctpa_denorm, use_training_transpose=False)
                ctpa_denorm_oriented = np.clip(ctpa_denorm_oriented, -1024, 3071)
                nifti_denorm = create_medical_nifti(ctpa_denorm_oriented)
                denorm_path = output_path / f"{filename}_denormalized_none_style.nii.gz"
                sitk.WriteImage(nifti_denorm, str(denorm_path))
                print(f"âœ… ÄÃ£ lÆ°u {filename}_denormalized_none_style.nii.gz (Ä‘á»ƒ so sÃ¡nh)")
        
        if 'png' in formats:
            # LÆ°u slices Ä‘áº¡i diá»‡n
            slice_dir = output_path / f"{filename}_slices"
            slice_dir.mkdir(exist_ok=True)
            
            # Sá»­ dá»¥ng denormalize_for_display Ä‘á»ƒ cÃ³ PNG Ä‘áº¹p
            ctpa_for_png = denormalize_for_display(ctpa_volume)
            print(f"ğŸ“Š CTPA for PNG range: [{ctpa_for_png.min():.3f}, {ctpa_for_png.max():.3f}]")
            
            num_slices = ctpa_for_png.shape[0]
            indices = np.linspace(0, num_slices-1, min(10, num_slices)).astype(int)
            
            for i, idx in enumerate(indices):
                slice_img = ctpa_for_png[idx]
                
                # Convert to 0-255 range for PNG
                slice_img_255 = (slice_img * 255).astype(np.uint8)
                
                cv2.imwrite(str(slice_dir / f"slice_{i:03d}.png"), slice_img_255)
            
            print(f"âœ… ÄÃ£ lÆ°u {len(indices)} slices PNG")
            
        if 'gif' in formats:
            # Táº¡o GIF animation GIá»NG Há»†T REPO Gá»C
            print("ğŸ”„ Äang táº¡o GIF animation GIá»NG Há»†T REPO Gá»C...")
            
            # Sá»­ dá»¥ng volume gá»‘c (khÃ´ng denormalize) Ä‘á»ƒ giá»¯ cháº¥t lÆ°á»£ng GIá»NG training
            ctpa_for_gif = ctpa_volume.copy()
            print(f"ğŸ“Š CTPA for GIF range: [{ctpa_for_gif.min():.3f}, {ctpa_for_gif.max():.3f}]")
            
            # Prepare tensor cho video_tensor_to_gif function
            # Training code expects (channels, frames, height, width)
            # ctpa_for_gif shape: (depth, height, width)
            # Chuyá»ƒn thÃ nh (1, depth, height, width) Ä‘á»ƒ match format GIá»NG training
            gif_tensor = torch.from_numpy(ctpa_for_gif).unsqueeze(0)  # Add channel dimension
            
            # Táº¡o GIF GIá»NG Há»†T repo gá»‘c vá»›i duration=120 (default)
            gif_path = output_path / f"{filename}_none_style.gif"
            video_tensor_to_gif(gif_tensor, str(gif_path), duration=120)
            print(f"âœ… ÄÃ£ táº¡o animation GIF GIá»NG Há»†T REPO Gá»C: {gif_path}")
    
    def _apply_medical_windowing(self, image: np.ndarray, center: float, width: float) -> np.ndarray:
        """Apply medical windowing cho visualization"""
        img_min = center - width / 2
        img_max = center + width / 2
        windowed = np.clip(image, img_min, img_max)
        # Normalize to 0-255
        windowed = ((windowed - img_min) / (img_max - img_min) * 255).astype(np.uint8)
        return windowed
    
    def inference_pipeline(
        self,
        xray_path: str,
        output_dir: str,
        filename: str = "generated_ctpa",
        guidance_scale: float = 1.0,
        show_viewer: bool = True,
        formats: list = ['npy', 'nii', 'png', 'gif']
    ) -> Tuple[np.ndarray, Optional[object]]:
        """
        Pipeline inference hoÃ n chá»‰nh GIá»NG Há»†T TRAINING CODE Gá»C
        """
        print(f"ğŸš€ Báº¯t Ä‘áº§u inference pipeline GIá»NG Há»†T TRAINING CODE...")
        print(f"ğŸ“‚ X-ray input: {xray_path}")
        print(f"ğŸ“ Output directory: {output_dir}")
        print(f"ğŸ“‹ Output formats: {formats}")
        print(f"ğŸ¯ Orientation: NO transpose - GIá»NG generated_ctpa_none")
        print(f"ğŸ¨ Cháº¥t lÆ°á»£ng: HU range [-1024, 3071] - giá»¯ cháº¥t lÆ°á»£ng nhÆ° training code")
        print(f"ğŸ¬ GIF: duration=120, normalization giá»‘ng repo gá»‘c")
        
        # 1. Preprocess X-ray
        xray_tensor = self.preprocess_xray(xray_path)
        print(f"âœ… Preprocessed X-ray shape: {xray_tensor.shape}")
        
        # 2. Generate CTPA
        ctpa_tensor = self.generate_ctpa(xray_tensor, guidance_scale)
        print(f"âœ… Generated CTPA shape: {ctpa_tensor.shape}")
        
        # 3. Postprocess
        ctpa_volume = self.postprocess_ctpa(ctpa_tensor)
        print(f"âœ… Final CTPA volume shape: {ctpa_volume.shape}")
        
        # 4. Save results vá»›i formats Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
        self.save_results(ctpa_volume, output_dir, filename, formats=formats)
        
        # 5. Create viewer náº¿u cÃ³ visualization_utils
        viewer = None
        if show_viewer:
            try:
                from visualization_utils import CTSliceViewer
                # Sá»­ dá»¥ng denormalize_for_display Ä‘á»ƒ consistency vá»›i PNG
                ctpa_for_viewer = denormalize_for_display(ctpa_volume)
                viewer = CTSliceViewer(ctpa_for_viewer)
                print("âœ… ÄÃ£ táº¡o CT viewer")
            except ImportError:
                print("âš ï¸ KhÃ´ng thá»ƒ import CTSliceViewer, bá» qua viewer")
        
        print("ğŸ‰ Inference pipeline hoÃ n thÃ nh GIá»NG generated_ctpa_none!")
        print(f"   - Orientation: NO transpose - GIá»NG generated_ctpa_none")
        print(f"   - Cháº¥t lÆ°á»£ng: HU range [-1024, 3071] - giá»¯ cháº¥t lÆ°á»£ng nhÆ° training code")
        print(f"   - GIF: duration=120, normalization giá»‘ng repo gá»‘c")
        if 'npy' in formats:
            print(f"   - {filename}_raw.npy: Raw volume data")
        if 'nii' in formats:
            print(f"   - {filename}_none_style.nii.gz: NIfTI GIá»NG generated_ctpa_none")
        if 'png' in formats:
            print(f"   - {filename}_slices/: PNG slices")
        if 'gif' in formats:
            print(f"   - {filename}_none_style.gif: GIF animation GIá»NG Há»†T repo gá»‘c")
        
        return ctpa_volume, viewer


def main():
    """HÃ m main Ä‘á»ƒ cháº¡y inference GIá»NG Há»†T TRAINING CODE"""
    parser = argparse.ArgumentParser(description="X-ray to CTPA Inference GIá»NG Há»†T TRAINING CODE Gá»C")
    parser.add_argument("--checkpoint", type=str, required=True, help="ÄÆ°á»ng dáº«n Ä‘áº¿n model checkpoint")
    parser.add_argument("--xray", type=str, required=True, help="ÄÆ°á»ng dáº«n Ä‘áº¿n áº£nh X-ray input")
    parser.add_argument("--output", type=str, default="./inference_results", help="ThÆ° má»¥c output")
    parser.add_argument("--filename", type=str, default="generated_ctpa", help="TÃªn file output")
    parser.add_argument("--guidance-scale", type=float, default=1.0, help="Guidance scale")
    parser.add_argument("--device", type=str, default="cuda", help="Device (cuda/cpu)")
    parser.add_argument("--show-viewer", action="store_true", help="Hiá»ƒn thá»‹ interactive viewer")
    
    args = parser.parse_args()
    
    # Cáº¥u hÃ¬nh model theo config Ä‘Ã£ cho
    model_config = {
        'denoising_fn': 'Unet3D',
        'diffusion_img_size': 32,
        'diffusion_depth_size': 128,  # Tá»« config
        'diffusion_num_channels': 4,
        'dim_mults': [1, 2, 4, 8],
        'cond_dim': 512,
        'timesteps': 1000,
        'loss_type': 'l1_lpips',  # Tá»« config
        'l1_weight': 1.0,
        'perceptual_weight': 0.01,  # Tá»« config
        'discriminator_weight': 0.0,
        'classification_weight': 0.0,
        'classifier_free_guidance': False,
        'medclip': True,
        'name_dataset': 'LIDC',  # Tá»« config
        'dataset_min_value': -12.911299,  # Tá»« config
        'dataset_max_value': 9.596558,   # Tá»« config
        'vae_ckpt': 'stabilityai/sd-vae-ft-mse-original',
        'vqgan_ckpt': None
    }
    
    # Khá»Ÿi táº¡o inference
    inference = XrayToCTPAInference(
        model_checkpoint=args.checkpoint,
        model_config=model_config,
        device=args.device
    )
    
    # Cháº¡y inference vá»›i settings GIá»NG generated_ctpa_none
    formats = ['npy', 'nii', 'png', 'gif']  # Táº¡o táº¥t cáº£ formats
    print("ğŸ”„ Mode: GIá»NG FILE generated_ctpa_none")
    print("   - Orientation: NO transpose nhÆ° generated_ctpa_none")
    print("   - Cháº¥t lÆ°á»£ng: HU range [-1024, 3071] nhÆ° training code")
    print("   - GIF: duration=120, normalization giá»‘ng repo gá»‘c")
    
    # Cháº¡y inference
    ctpa_volume, viewer = inference.inference_pipeline(
        xray_path=args.xray,
        output_dir=args.output,
        filename=args.filename,
        guidance_scale=args.guidance_scale,
        show_viewer=args.show_viewer
    )
    
    # Hiá»ƒn thá»‹ viewer náº¿u Ä‘Æ°á»£c yÃªu cáº§u
    if args.show_viewer and viewer:
        print("ğŸ–¥ï¸  Hiá»ƒn thá»‹ interactive viewer...")
        viewer.show_interactive_viewer()


if __name__ == "__main__":
    main() 