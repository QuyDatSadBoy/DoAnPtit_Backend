#!/usr/bin/env python3
"""
Fast Inference Wrapper - T·ªëi ∆∞u t·ªëc ƒë·ªô m√† KH√îNG thay ƒë·ªïi code g·ªëc
Ch·ªâ c·∫ßn ch·∫°y script n√†y thay v√¨ inference.py ƒë·ªÉ c√≥ t·ªëc ƒë·ªô nhanh h∆°n

C√°ch d√πng:
    python inference_fast.py --checkpoint ./checkpoints/model-81.pt --xray "./x-ray input/LIDC-IDRI-0001.npy" --output ./results --filename test

C√°c t·ªëi ∆∞u ƒë∆∞·ª£c √°p d·ª•ng:
1. cudnn.benchmark = True - T·ª± ƒë·ªông t√¨m thu·∫≠t to√°n nhanh nh·∫•t
2. TF32 enabled - TƒÉng t·ªëc tr√™n GPU Ampere/Ada/Hopper
3. Memory optimization - D√πng nhi·ªÅu VRAM h∆°n
"""

import os
import sys
import torch

# ============================================
# CUDA OPTIMIZATIONS - √Åp d·ª•ng TR∆Ø·ªöC khi import model
# ============================================
print("üîß Applying CUDA optimizations...")

# 1. cudnn.benchmark - T√¨m thu·∫≠t to√°n nhanh nh·∫•t cho input size c·ªë ƒë·ªãnh
torch.backends.cudnn.benchmark = True

# 2. TF32 - TƒÉng t·ªëc matmul tr√™n RTX 30xx/40xx/50xx
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# 3. Memory optimization - Cho ph√©p d√πng nhi·ªÅu VRAM h∆°n
if torch.cuda.is_available():
    # D√πng memory allocator hi·ªáu qu·∫£ h∆°n
    torch.cuda.empty_cache()
    
print("‚úÖ CUDA optimizations applied!")
print("   - cudnn.benchmark = True")
print("   - TF32 enabled for matmul")
print("   - Memory optimized")
print("")

# ============================================
# Ch·∫°y inference.py g·ªëc v·ªõi c√°c t·ªëi ∆∞u ƒë√£ √°p d·ª•ng
# ============================================

# Import v√† ch·∫°y main t·ª´ inference.py
if __name__ == "__main__":
    # Th√™m th∆∞ m·ª•c hi·ªán t·∫°i v√†o path
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    # Import v√† ch·∫°y main t·ª´ inference.py
    from inference import main
    main()
